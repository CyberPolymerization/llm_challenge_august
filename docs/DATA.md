

This LLM challenge is about building question/answering capabilities with LLM about ADI datasheets.
To this end, we create a corpus of Question and Answer (Q&A) pairs from a collection of public ADI datasheets/documents that our colleagues Matt Crivello, Andrew Fitzell, Marc Light, and Dave Boland curated.

In the following, we discuss
1. [ADI Datasheets](#adisheets)
2. [Question/Answers (Q&As) Datasets](#qa-dset)
3. [Submission Format](#sub-format)
4. [Data From Tutorial](#tutorial-data)

---
#### ADI Datasheets <a name="adisheets"></a>

The full curated dataset of selected ADI datasheets can be found [here in datasheets.zip](https://analog-my.sharepoint.com/:f:/p/ash_aldujaili/En4O_6p0cLJBhsgtsNmQQFAByQed_Z8Y4LNuyqJJbvFAMQ?e=LXYzP1). 

It has the following structure:
```
.
├── analyzed // contains JSON files encoding a structured representation of `raw` pdf files (e.g., general description (gd), features)
├── embeddings // LLM embeddings of `analyzed` JSON files items.
├── parsed // contains extracted text from `raw` pdf files
└── raw // contains pdf files of ADI datasheets
```
For the purpose of this challenge, contents of the `parsed` directory might be sufficient.
But we encourage participants to explore ways of using the rest of the dataset.

We thank Matt Crivello, Andrew Fitzell, Marc Light, and Dave Boland for sharing this corpus of data with us.

---
#### Question/Answers (Q&As) Datasets <a name="qa-dset"></a>


We created two sets (`train` and `test`) of question/answer pairs extracted from the dataset described above. 
Participants can use the `train` dataset for developing their solutions.
We will use the `test` set for evaluating/ranking participants' submissions.


For each set of Q&As, there are two JSON files:
1. `qas_{train|test}.json`: stores a table/dictionary of Q&As indexed by their ID in the following format.

```
"$ID":
    {
        "question": "$QUESTION",
        "answer": "$ANSWER",
        "datasheet": "$PATH_TO_TEXT_CONTAINING_THE_ANSWER"
    },

```
2. `contexts_{train|test}.json`

```
{
    "$ID": "$SNIPPET_OF_TEXT_FROM_DATASHEET_CONTAINING_THE_ANSWER_TO_QUESTION[$ID]"
}
```

For convenience, we provide a helper Python function to read these JSON files. For example,
```
from llm_challenge.utils.misc import read_dict_from_json
qas_dict = read_dict_from_json(PATH_TO_QA_JSON_FILE)
contexts_dict = read_dict_from_json(PATH_TO_CONTEXT_FILE)
```

*Note 1*: For the purpose of tutorials/going through the notebook, we extracted a subset of `train` Q&As (5 questions and answers) that are discussed in the notebooks and generated their corresponding files. That is, you will also find `qas_tutorial.json` and `contexts_tutorial.json`.

*Note 2*: during the challenge, `qas_test.json` and `contexts_test.json` wont be shared with the participants. Instead, we will share `qas_wo_answers_test.json`--a
redacted version of `qas_test.json`, where answers to all the questions are "I do not know.". This is a safe-guard to ensure fair evaluation to all entries. We will share the `qas_test.json` after the challenge concludes.

The JSON files discussed above can be found [here in qas.zip](https://analog-my.sharepoint.com/:f:/p/ash_aldujaili/En4O_6p0cLJBhsgtsNmQQFAByQed_Z8Y4LNuyqJJbvFAMQ?e=LXYzP1).

---
#### Submission Format <a name="sub-format"></a>

For evaluation/ranking purposes, we suggest participants send us their entries in a JSON file with the following format:

```
{
    "$ID": "$PARTICIAPNT_ENTRY_ANSWER_TO_QUESTION[$ID]"
}
```


For convenience, we provide a helper Python function to write the submission into a JSON file. For example,
```
from llm_challenge.utils.misc import write_dict_to_json
write_dict_to_json("./{$YOUR_SUBMISSION_NAME}_train.json", answers_dict)
```

We recommend the following naming convention to your JSON submission files: `{$YOUR_SUBMISSION_NAME}_{$DSET}.json`
$YOUR_SUBMISSION_NAME can be any name you like--your team's name, your method's name, etc.
$DSET can be `train`,`tutorial` or `test` if you're answering `train`, `tutorial` or `test` questions, respectively.

Examples of submission files can be found [here in llm_challenge_submission_examples.zip](https://analog-my.sharepoint.com/:f:/p/ash_aldujaili/En4O_6p0cLJBhsgtsNmQQFAByQed_Z8Y4LNuyqJJbvFAMQ?e=LXYzP1).

---
#### Data From Tutorial <a name="tutorial-data"></a>

To save participants' token on creating embeddings/vector stores on both train (and the subset of train - tutorial) and test datasets, we share the files that were generated during the tutorials, namely:

- `embeddings_train.npz`, `embeddings_tutorial.npz` and `embeddings_test.npz`: these are the embedding npz files generated by the `nb02-fetching-context-for-llm` notebook.
- `db_train`, `db_tutorial` and `db_test` directories: these are the persist directories for train and test datasets respectively generated by the  `nb03-qa-with-vector-store` notebook.

These files can be found within qas.zip files linked [above](#qa-dset). Just download these files to your disk and update the notebooks' paths to point to where these files/directories are downloaded (we recommend putting them in a `data` directory within the top-level directory of this code base).
