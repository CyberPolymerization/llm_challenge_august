{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73578aec",
   "metadata": {},
   "source": [
    "An example of using Chroma DB and LangChain to do question answering over documents, with a locally persisted database. You can store embeddings and documents, then use them again later.\n",
    "\n",
    "This notebook is adapted from: https://github.com/hwchase17/chroma-langchain/blob/master/persistent-qa.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb8b60d4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'llm_challenge'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mchains\u001b[39;00m \u001b[39mimport\u001b[39;00m RetrievalQA\n\u001b[1;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdocument_loaders\u001b[39;00m \u001b[39mimport\u001b[39;00m TextLoader\n\u001b[0;32m----> 8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mllm_challenge\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmisc\u001b[39;00m \u001b[39mimport\u001b[39;00m read_dict_from_json, set_openai_api_key, write_dict_to_json\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpathlib\u001b[39;00m \u001b[39mimport\u001b[39;00m Path\n\u001b[1;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtqdm\u001b[39;00m \u001b[39mimport\u001b[39;00m tqdm\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'llm_challenge'"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "from llm_challenge.utils.misc import read_dict_from_json, set_openai_api_key, write_dict_to_json\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4703dec",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'set_openai_api_key' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m set_openai_api_key()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'set_openai_api_key' is not defined"
     ]
    }
   ],
   "source": [
    "set_openai_api_key()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52d940b",
   "metadata": {},
   "source": [
    "Load and process documents\n",
    "Load documents to do question answering over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30210553",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = \"tutorial\"\n",
    "DATA_DIR = \"../data\"\n",
    "qas_dict = read_dict_from_json(f\"{DATA_DIR}/qas/qas_{dset}.json\")\n",
    "doc_fnames = list(set([str(DATA_DIR + \"/datasheets/\" + datum[\"datasheet\"])  for datum in qas_dict.values()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "855820c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and process the text\n",
    "loaders = [TextLoader(_) for _ in doc_fnames]\n",
    "documents = [d for loader in loaders for d in loader.load()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0476fe90",
   "metadata": {},
   "source": [
    "Next we split documents into small chunks. This is so we can find the most relevant chunks for a query and pass only those into the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96a3a90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfafd1d",
   "metadata": {},
   "source": [
    "Initialize PeristedChromaDB\n",
    "\n",
    "Create embeddings for each chunk and insert into the Chroma vector database. The persist_directory argument tells ChromaDB where to store the database when it's persisted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e8286fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supplying a persist_directory will store the embeddings on disk\n",
    "persist_directory = f'{DATA_DIR}/qas/db_{dset}'\n",
    "embedding = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2a496bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database exists... loading from disk\n"
     ]
    }
   ],
   "source": [
    "# Embed and store the texts\n",
    "# Persist the Database\n",
    "# In a notebook, we should call persist() to ensure the embeddings are written to disk. \n",
    "# This isn't necessary in a script - the database will be automatically persisted when the client object is destroyed.\n",
    "\n",
    "if Path(persist_directory).exists():\n",
    "    print(\"Database exists... loading from disk\")\n",
    "    # Load the Database from disk, and create the chain\n",
    "    # Be sure to pass the same persist_directory and embedding_function as \n",
    "    # you did when you instantiated the database. Initialize the chain we will use for question answering.\n",
    "    # Now we can load the persisted database from disk, and use it as normal. \n",
    "    vectordb = Chroma(persist_directory=persist_directory, embedding_function=embedding)\n",
    "else:\n",
    "    print(\"Database does not exist. Do you want to recompute it?\")\n",
    "    print(\"If yes, set `is_generate_db` to True in the next cell.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "410bcb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to save from token-expensive ops (set to False)\n",
    "is_generate_db = False\n",
    "if is_generate_db:\n",
    "    vectordb = Chroma.from_documents(documents=texts, embedding=embedding, persist_directory=persist_directory)\n",
    "    vectordb.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e9f4a2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the retrieval QA chain with `stuff` mode\n",
    "qa = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type=\"stuff\", retriever=vectordb.as_retriever())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7dca41",
   "metadata": {},
   "source": [
    "Ask questions!\n",
    "\n",
    "Now we can use the chain to ask questions!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1c387386",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Answering question 50: 100%|███████████████████████████████████████████████████████████████████████| 5/5 [00:14<00:00,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens Used: 6334\n",
      "\tPrompt Tokens: 6205\n",
      "\tCompletion Tokens: 129\n",
      "Successful Requests: 5\n",
      "Total Cost (USD): $0.12668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "answers_with_retrieval = {}\n",
    "with get_openai_callback() as cb:\n",
    "    for q_id, qa_dict in (pbar := tqdm(qas_dict.items())):\n",
    "        pbar.set_description(f\"Answering question {q_id}\")\n",
    "        answer = qa.run(qa_dict[\"question\"])\n",
    "        answers_with_retrieval[q_id] = answer\n",
    "    print(cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c5c82d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$0.12668\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/hwchase17/langchain/blob/57f370cde97fb3f2c836cbc0dc764be55a571d5a/langchain/callbacks/openai_info.py#L7\n",
    "# text-davinci-003\n",
    "COST_PER_1K_TOKENS_DAV = 0.02\n",
    "print(f\"${6334 * COST_PER_1K_TOKENS_DAV * 1e-3:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c5c286",
   "metadata": {},
   "source": [
    "This is more expensive than the approach discussed in nb02 though this approach uses less tokens:\n",
    "```\n",
    "        NUMPY   | LANGCHAIN\n",
    "tokens: 37798   |  6334\n",
    "cost  : $0.0416 | $0.126\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f00a9f5",
   "metadata": {},
   "source": [
    "Store the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "650aa9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_dict_to_json(f\"./langchainRetriever_{dset}.json\",answers_with_retrieval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb25daaa",
   "metadata": {},
   "source": [
    "Evaluating with\n",
    "\n",
    "```\n",
    "python scripts/evaluate.py --submission_json_path ./notebooks/langchainRetriever_tutorial.json --reference_json_fname ./data/qas/qas_tutorial.json --submission_evaluation_json_fname ./langchainRetriever_evaluation.json\n",
    "```\n",
    "\n",
    "```\n",
    "python scripts/evaluate.py --submission_json_path ./notebooks/langchainRetriever_train.json --reference_json_fname ./data/qas/qas_train.json --submission_evaluation_json_fname ./langchainRetriever_evaluation.json\n",
    "```\n",
    "gets us 73.3 %"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
